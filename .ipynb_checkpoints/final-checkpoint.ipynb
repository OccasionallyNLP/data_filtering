{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aede3692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b02ae784",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [i for i in os.listdir('/home/work/user/wjpark/midm_pack/data/')]\n",
    "data_list_ = []\n",
    "for i in data_list:\n",
    "    for j in list(map(str, range(2013, 2024))):\n",
    "        if j in i:\n",
    "            if 'test' not in i:\n",
    "                \n",
    "                data_list_.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c23cbb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2014-23 2021-17 2020-40 2021-04 2016-26 2015-22 2017-09 2018-05 2018-43 2017-43 2021-21 2023-06 2021-31 2019-39 2015-35 2019-13 2017-51 2016-36 2019-47 2020-10 2017-17 2019-43 2017-04 2022-21 2018-34 2018-22 2021-39 2014-10 2022-27 2015-48 2018-09 2022-40 2016-18 2019-51 2018-51 2021-43 2013-20 2013-48 2019-09 2017-47 2017-26 2018-17 2016-22 2016-40 2021-49 2019-22 2019-18 2019-30 2018-47 2016-07 2018-30 2017-22 2023-14 2017-30 2023-23 2020-50 2017-34 2018-26 2014-49 2020-45 2022-05 2018-39 2020-29 2023-40 2020-16 2014-35 2019-35 2015-32 2019-26 2014-15 2018-13 2022-33 2016-44 2017-39 2020-05 2016-30 2021-25 2021-10 2020-34 2020-24 2016-50 2023-50 2019-04'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(data_list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77f48094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467349bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path, verbose=False):\n",
    "    result = []\n",
    "    f = open(path,'r',encoding = 'utf-8')\n",
    "    len(f)\n",
    "    for i in tqdm(f, disable=not verbose):\n",
    "        try:\n",
    "            result.append(json.loads(i))\n",
    "        except:\n",
    "            continue\n",
    "    return result \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd45e1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1% 1/80 [03:54<5:08:21, 234.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before : 130 -------> after : 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2% 2/80 [04:09<2:17:08, 105.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before : 12 -------> after : 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4% 3/80 [04:26<1:23:30, 65.08s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before : 14 -------> after : 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5% 4/80 [05:48<1:30:46, 71.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before : 56 -------> after : 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6% 5/80 [08:29<2:09:55, 103.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before : 118 -------> after : 118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8% 6/80 [12:38<3:08:53, 153.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before : 162 -------> after : 162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9% 7/80 [15:22<3:10:48, 156.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before : 108 -------> after : 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10% 8/80 [17:12<2:50:21, 141.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before : 70 -------> after : 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11% 9/80 [21:36<3:33:11, 180.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before : 176 -------> after : 176\n"
     ]
    }
   ],
   "source": [
    "for name in tqdm(data_list_[3:]):\n",
    "    path = f'/home/work/user/wjpark/midm_pack/data/{name}/remove_duplicate'\n",
    "    before = os.listdir(path)\n",
    "    for i in os.listdir(path):\n",
    "        if i.endswith('part'):\n",
    "            data = load_jsonl(os.path.join(path,i), False)\n",
    "            if len(data)<=1:\n",
    "                os.remove(os.path.join(path,i))\n",
    "        else:\n",
    "            os.remove(os.path.join(path,i))\n",
    "    after = os.listdir(path)\n",
    "    print(f'before : {len(before)} -------> after : {len(after)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcaa6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './upload/test.txt'\n",
    "\n",
    "if os.path.isfile(file):\n",
    "  os.remove(file)\n",
    "\n",
    "  return 'okay'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a2f0fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi\n",
    "tokenizer = Kiwi(model_type='sbg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "edc8369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(text):\n",
    "    tokens = [token.form for token in tokenizer.tokenize(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b7c43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27% 7416/27583 [08:43<19:55, 16.87it/s]  "
     ]
    }
   ],
   "source": [
    "for i in tqdm(data):\n",
    "    word_tokenize(i['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c160a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8868098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatrove.pipeline.filters import (\n",
    "    C4QualityFilter,\n",
    "    FineWebQualityFilter,\n",
    "    GopherQualityFilter,\n",
    "    GopherRepetitionFilter,\n",
    "    LanguageFilter,\n",
    "    URLFilter,\n",
    ")\n",
    "\n",
    "from typing import Callable, Literal\n",
    "from datatrove.io import DataFileLike, DataFolderLike\n",
    "from datatrove.pipeline.readers.base import BaseDiskReader\n",
    "from datatrove.utils.logging import logger\n",
    "\n",
    "\n",
    "class JsonlReader(BaseDiskReader):\n",
    "    \"\"\"Read data from JSONL files.\n",
    "        Will read each line as a separate document.\n",
    "\n",
    "    Args:\n",
    "        data_folder: a str, tuple or DataFolder object representing a path/filesystem\n",
    "        paths_file: optionally provide a file with one path per line (without the `data_folder` prefix) to read.\n",
    "        compression: the compression to use (default: \"infer\")\n",
    "        limit: limit the number of documents to read. Useful for debugging\n",
    "        skip: skip the first n rows\n",
    "        file_progress: show progress bar for files\n",
    "        doc_progress: show progress bar for documents\n",
    "        adapter: function to adapt the data dict from the source to a Document.\n",
    "            Takes as input: (self, data: dict, path: str, id_in_file: int | str)\n",
    "                self allows access to self.text_key and self.id_key\n",
    "            Returns: a dict with at least a \"text\" and \"id\" keys\n",
    "        text_key: the key containing the text data (default: \"text\").\n",
    "        id_key: the key containing the id for each sample (default: \"id\").\n",
    "        default_metadata: a dictionary with any data that should be added to all samples' metadata\n",
    "        recursive: whether to search files recursively. Ignored if paths_file is provided\n",
    "        glob_pattern: pattern that all files must match exactly to be included (relative to data_folder). Ignored if paths_file is provided\n",
    "        shuffle_files: shuffle the files within the returned shard. Mostly used for data viz. purposes, do not use with dedup blocks\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"üêø Jsonl\"\n",
    "    _requires_dependencies = [\"orjson\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_folder: DataFolderLike,\n",
    "        paths_file: DataFileLike | None = None,\n",
    "        compression: Literal[\"infer\", \"gzip\", \"zstd\"] | None = \"infer\",\n",
    "        limit: int = -1,\n",
    "        skip: int = 0,\n",
    "        file_progress: bool = False,\n",
    "        doc_progress: bool = False,\n",
    "        adapter: Callable = None,\n",
    "        text_key: str = \"text\",\n",
    "        id_key: str = \"id\",\n",
    "        default_metadata: dict = None,\n",
    "        recursive: bool = True,\n",
    "        glob_pattern: str | None = None,\n",
    "        shuffle_files: bool = False,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            data_folder,\n",
    "            paths_file,\n",
    "            limit,\n",
    "            skip,\n",
    "            file_progress,\n",
    "            doc_progress,\n",
    "            adapter,\n",
    "            text_key,\n",
    "            id_key,\n",
    "            default_metadata,\n",
    "            recursive,\n",
    "            glob_pattern,\n",
    "            shuffle_files,\n",
    "        )\n",
    "        self.compression = compression\n",
    "\n",
    "    def read_file(self, filepath: str):\n",
    "        import orjson\n",
    "        from orjson import JSONDecodeError\n",
    "\n",
    "        with self.data_folder.open(filepath, \"r\", compression=self.compression) as f:\n",
    "            try:\n",
    "                for li, line in enumerate(f):\n",
    "                    with self.track_time():\n",
    "                        try:\n",
    "                            document = self.get_document_from_dict(orjson.loads(line), filepath, li)\n",
    "                            document.text=document.text.replace('<[!newline]>\\n','\\n')\n",
    "                            if not document:\n",
    "                                continue\n",
    "                        except (EOFError, JSONDecodeError) as e:\n",
    "                            logger.warning(f\"Error when reading `{filepath}`: {e}\")\n",
    "                            continue\n",
    "                    yield document\n",
    "            except UnicodeDecodeError as e:\n",
    "                logger.warning(f\"File `{filepath}` may be corrupted: raised UnicodeDecodeError ({e})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1038f4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = [os.path.join(path,i) for i in os.listdir(path) if i.endswith('part')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d693c592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatrove.data import Document\n",
    "import datatrove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5381077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filter(filter_for_fineweb, dataset:datatrove.data.Document):\n",
    "    after = []\n",
    "    filtered = []\n",
    "    whys = []\n",
    "    for i in tqdm(dataset):\n",
    "        if filter_for_fineweb.filter(i)==True:\n",
    "            after.append(i)\n",
    "        else:\n",
    "            why = str(filter_for_fineweb.filter(i))\n",
    "            whys.append(why)\n",
    "            filtered.append(i)\n",
    "    print(f'size of dataset before filtering - {len(dataset)}')\n",
    "    print(f'size of dataset after filtering - {len(after)}')\n",
    "    return after, filtered, whys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a030fb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9455it [00:00, 38978.95it/s]\n"
     ]
    }
   ],
   "source": [
    "for data_name in data_list[:1]:\n",
    "    data = load_jsonl(data_name)\n",
    "    for i in data:\n",
    "        i['text']=i['text'].replace('<[!newline]>\\n','\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba1ec5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/user/wjpark/midm_pack/data/2021-17/remove_duplicate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-12 07:49:37.505 | INFO     | datatrove.utils.logging:add_task_logger:58 - Launching pipeline for rank=8\n",
      "2024-08-12 07:49:37.505 | INFO     | datatrove.utils.logging:log_pipeline:90 - \n",
      "--- üõ†Ô∏è PIPELINE üõ†\n",
      "üìñ - READER: üêø Jsonl\n",
      "üîª - FILTER: üòà Url-filter\n",
      "üîª - FILTER: üëØ Gopher Repetition\n",
      "üíΩ - WRITER: üêø Jsonl\n",
      "2024-08-12 07:49:37.543 | INFO     | datatrove.pipeline.readers.base:read_files_shard:191 - Reading input file 105.jsonl.part, 1/1\n",
      "2024-08-12 07:49:52.951 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 1/110 tasks completed.\n",
      "2024-08-12 07:49:52.954 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 2/110 tasks completed.\n",
      "Process ForkServerPoolWorker-19:\n",
      "2024-08-12 07:49:52.958 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 3/110 tasks completed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/executor/local.py\", line 76, in _launch_run_for_rank\n",
      "    return self._run_for_rank(rank, local_rank)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/executor/base.py\", line 96, in _run_for_rank\n",
      "    deque(pipelined_data, maxlen=0)\n",
      "Process ForkServerPoolWorker-24:\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/writers/disk_base.py\", line 176, in run\n",
      "    for document in data:\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/filters/base_filter.py\", line 68, in run\n",
      "    batch_filter_result = self.filter_batch(batch)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/filters/base_filter.py\", line 60, in filter_batch\n",
      "    return list(map(self.filter, batch))\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/filters/gopher_repetition_filter.py\", line 127, in filter\n",
      "    words = self.tokenizer.word_tokenize(text)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/utils/word_tokenizers.py\", line 206, in word_tokenize\n",
      "    tokens = [token.form for token in self.tokenizer.tokenize(text)]\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/kiwipiepy/_wrap.py\", line 1278, in tokenize\n",
      "    return self._tokenize(text, match_options, normalize_coda, z_coda, split_complex, split_sents, stopwords, echo,\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/kiwipiepy/_wrap.py\", line 1096, in _tokenize\n",
      "    return _refine_result(super().analyze(text, 1, match_options, False, blocklist, pretokenized))\n",
      "KeyboardInterrupt\n",
      "2024-08-12 07:49:52.959 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 4/110 tasks completed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/executor/local.py\", line 76, in _launch_run_for_rank\n",
      "    return self._run_for_rank(rank, local_rank)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/executor/base.py\", line 96, in _run_for_rank\n",
      "    deque(pipelined_data, maxlen=0)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/writers/disk_base.py\", line 176, in run\n",
      "    for document in data:\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/filters/base_filter.py\", line 68, in run\n",
      "    batch_filter_result = self.filter_batch(batch)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/filters/base_filter.py\", line 60, in filter_batch\n",
      "    return list(map(self.filter, batch))\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/filters/gopher_repetition_filter.py\", line 127, in filter\n",
      "    words = self.tokenizer.word_tokenize(text)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/utils/word_tokenizers.py\", line 206, in word_tokenize\n",
      "    tokens = [token.form for token in self.tokenizer.tokenize(text)]\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/kiwipiepy/_wrap.py\", line 1278, in tokenize\n",
      "    return self._tokenize(text, match_options, normalize_coda, z_coda, split_complex, split_sents, stopwords, echo,\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/kiwipiepy/_wrap.py\", line 1096, in _tokenize\n",
      "    return _refine_result(super().analyze(text, 1, match_options, False, blocklist, pretokenized))\n",
      "KeyboardInterrupt\n",
      "Process ForkServerPoolWorker-23:\n",
      "Process ForkServerPoolWorker-26:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/executor/local.py\", line 76, in _launch_run_for_rank\n",
      "    return self._run_for_rank(rank, local_rank)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/executor/base.py\", line 96, in _run_for_rank\n",
      "    deque(pipelined_data, maxlen=0)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/writers/disk_base.py\", line 176, in run\n",
      "    for document in data:\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/filters/base_filter.py\", line 68, in run\n",
      "    batch_filter_result = self.filter_batch(batch)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/filters/base_filter.py\", line 60, in filter_batch\n",
      "    return list(map(self.filter, batch))\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/filters/gopher_repetition_filter.py\", line 127, in filter\n",
      "    words = self.tokenizer.word_tokenize(text)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/utils/word_tokenizers.py\", line 206, in word_tokenize\n",
      "    tokens = [token.form for token in self.tokenizer.tokenize(text)]\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/kiwipiepy/_wrap.py\", line 1278, in tokenize\n",
      "    return self._tokenize(text, match_options, normalize_coda, z_coda, split_complex, split_sents, stopwords, echo,\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/kiwipiepy/_wrap.py\", line 1096, in _tokenize\n",
      "    return _refine_result(super().analyze(text, 1, match_options, False, blocklist, pretokenized))\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/executor/local.py\", line 76, in _launch_run_for_rank\n",
      "    return self._run_for_rank(rank, local_rank)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/executor/base.py\", line 96, in _run_for_rank\n",
      "    deque(pipelined_data, maxlen=0)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/writers/disk_base.py\", line 176, in run\n",
      "    for document in data:\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/filters/base_filter.py\", line 64, in run\n",
      "    for batch in batched(data, self.batch_size):\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/utils/batching.py\", line 20, in batched\n",
      "    while batch := list(itertools.islice(it, n)):\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/filters/base_filter.py\", line 68, in run\n",
      "    batch_filter_result = self.filter_batch(batch)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/filters/base_filter.py\", line 60, in filter_batch\n",
      "    return list(map(self.filter, batch))\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/filters/url_filter.py\", line 111, in filter\n",
      "    url_info = self.tldextractor(url)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/tldextract/tldextract.py\", line 227, in __call__\n",
      "    return self.extract_str(url, include_psl_private_domains, session=session)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/tldextract/tldextract.py\", line 256, in extract_str\n",
      "    return self._extract_netloc(\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/tldextract/tldextract.py\", line 307, in _extract_netloc\n",
      "    suffix_index, is_private = self._get_tld_extractor(\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/tldextract/tldextract.py\", line 494, in suffix_index\n",
      "    if decoded_label in node.matches:\n",
      "KeyboardInterrupt\n",
      "2024-08-12 07:49:52.967 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 5/110 tasks completed.\n",
      "2024-08-12 07:49:52.969 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 6/110 tasks completed.\n",
      "Process ForkServerPoolWorker-30:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/executor/local.py\", line 76, in _launch_run_for_rank\n",
      "    return self._run_for_rank(rank, local_rank)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/executor/base.py\", line 96, in _run_for_rank\n",
      "    deque(pipelined_data, maxlen=0)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/writers/disk_base.py\", line 176, in run\n",
      "    for document in data:\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/filters/base_filter.py\", line 68, in run\n",
      "    batch_filter_result = self.filter_batch(batch)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/filters/base_filter.py\", line 60, in filter_batch\n",
      "    return list(map(self.filter, batch))\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/filters/gopher_repetition_filter.py\", line 127, in filter\n",
      "    words = self.tokenizer.word_tokenize(text)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/utils/word_tokenizers.py\", line 206, in word_tokenize\n",
      "    tokens = [token.form for token in self.tokenizer.tokenize(text)]\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/kiwipiepy/_wrap.py\", line 1278, in tokenize\n",
      "    return self._tokenize(text, match_options, normalize_coda, z_coda, split_complex, split_sents, stopwords, echo,\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/kiwipiepy/_wrap.py\", line 1096, in _tokenize\n",
      "    return _refine_result(super().analyze(text, 1, match_options, False, blocklist, pretokenized))\n",
      "KeyboardInterrupt\n",
      "Process ForkServerPoolWorker-22:\n",
      "2024-08-12 07:49:52.976 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 7/110 tasks completed.\n",
      "2024-08-12 07:49:52.979 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 8/110 tasks completed.\n",
      "Process ForkServerPoolWorker-32:\n",
      "2024-08-12 07:49:52.985 | INFO     | datatrove.executor.local:_launch_run_for_rank:81 - 9/110 tasks completed.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/multiprocess/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/multiprocess/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/multiprocess/pool.py\", line 125, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/executor/local.py\", line 76, in _launch_run_for_rank\n",
      "    return self._run_for_rank(rank, local_rank)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/executor/base.py\", line 96, in _run_for_rank\n",
      "    deque(pipelined_data, maxlen=0)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/writers/disk_base.py\", line 176, in run\n",
      "    for document in data:\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/filters/base_filter.py\", line 68, in run\n",
      "    batch_filter_result = self.filter_batch(batch)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/filters/base_filter.py\", line 60, in filter_batch\n",
      "    return list(map(self.filter, batch))\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/pipeline/filters/gopher_repetition_filter.py\", line 127, in filter\n",
      "    words = self.tokenizer.word_tokenize(text)\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/datatrove/utils/word_tokenizers.py\", line 206, in word_tokenize\n",
      "    tokens = [token.form for token in self.tokenizer.tokenize(text)]\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/kiwipiepy/_wrap.py\", line 1278, in tokenize\n",
      "    return self._tokenize(text, match_options, normalize_coda, z_coda, split_complex, split_sents, stopwords, echo,\n",
      "  File \"/home/work/.local/lib/python3.10/site-packages/kiwipiepy/_wrap.py\", line 1096, in _tokenize\n",
      "    return _refine_result(super().analyze(text, 1, match_options, False, blocklist, pretokenized))\n",
      "KeyboardInterrupt\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "    URLFilter(),\n",
    "    GopherRepetitionFilter(language='ko'),\n",
    "    GopherQualityFilter(min_stop_words=None,language='ko'),\n",
    "    C4QualityFilter(filter_no_terminal_punct=False,language='ko')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fb6373",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.1 (NGC 23.09/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
